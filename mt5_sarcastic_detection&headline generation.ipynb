{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGfPXb0YQlyo"
   },
   "source": [
    "# **Telugu To Gujarati**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2jL7-3qRBjs"
   },
   "outputs": [],
   "source": [
    "# **1.2) IndicTrans2**\n",
    "## Setup\n",
    "\n",
    "# Please run the cells below to install the necessary dependencies.\n",
    "\n",
    "%%capture\n",
    "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS8BOX2dRQ44"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd /content/IndicTrans2/huggingface_interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT5-R02BRoRE"
   },
   "source": [
    "## **Please restart session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnJPOYd_Rlvj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
    "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
    "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
    "!python3 -m pip install sentencepiece\n",
    "!git clone https://github.com/VarunGumma/IndicTransTokenizer\n",
    "%cd IndicTransTokenizer\n",
    "!python3 -m pip install --editable ./\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LDGX9nlRWek"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from IndicTransTokenizer import IndicProcessor, IndicTransTokenizer\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "quantization = None\n",
    "def initialize_model_and_tokenizer(ckpt_dir, direction, quantization):\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    tokenizer = IndicTransTokenizer(direction=direction)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig == None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
    "    translations = []\n",
    "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
    "        print(i)\n",
    "        batch = input_sentences[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Preprocess the batch and extract entity mappings\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "\n",
    "        # Tokenize the batch and generate input encodings\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            src=True,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Generate translations using the model\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "            )\n",
    "\n",
    "        # Decode the generated tokens into text\n",
    "        generated_tokens = tokenizer.batch_decode(generated_tokens.detach().cpu().tolist(), src=False)\n",
    "\n",
    "        # Postprocess the translations, including entity replacement\n",
    "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_dIVITNSlUH"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# def process_csv(input_csv_path, output_csv_path):\n",
    "    # Lists to store text and title from the input CSV\n",
    "text_list = []\n",
    "title_list = []\n",
    "\n",
    "# Read data from the input CSV and populate the lists\n",
    "with open('telugu_non_sar_test.csv', 'r', newline='', encoding='utf-8') as input_file:\n",
    "  reader = csv.DictReader(input_file)\n",
    "  for row in reader:\n",
    "      text_list.append(row['text'])\n",
    "      title_list.append(row['title'])\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "522cJtvdSotm"
   },
   "outputs": [],
   "source": [
    "# process_csv('telugu_sarcastic_train.csv', 'output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCgyqsGyUy_c",
    "outputId": "67629a39-4751-4cf8-8c84-9c5739b0701e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "print(len(text_list))\n",
    "print(len(title_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4ygpTrpRwjw",
    "outputId": "3bf0e91a-bc03-420c-a2ea-0271a2c71e67"
   },
   "outputs": [],
   "source": [
    "### Indic to Indic Example\n",
    "\n",
    "\n",
    "indic_indic_ckpt_dir = \"ai4bharat/indictrans2-indic-indic-1B\"  # ai4bharat/indictrans2-indic-indic-dist-320M\n",
    "indic_indic_tokenizer, indic_indic_model = initialize_model_and_tokenizer(indic_indic_ckpt_dir, \"indic-indic\", quantization)\n",
    "\n",
    "ip = IndicProcessor(inference=True)\n",
    "\n",
    "tgt_lang, src_lang = \"guj_Gujr\", \"tel_Telu\"\n",
    "tl_to_gu_text = batch_translate(text_list, src_lang, tgt_lang, indic_indic_model, indic_indic_tokenizer, ip)\n",
    "tl_to_gu_title = batch_translate(title_list, src_lang, tgt_lang, indic_indic_model, indic_indic_tokenizer, ip)\n",
    "\n",
    "# flush the models to free the GPU memory\n",
    "del indic_indic_tokenizer, indic_indic_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFW5jNc5Za-e",
    "outputId": "d60c62fa-a5ff-45c4-df90-670fdd5ee097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['માતા તમને કંઈક મોકલી રહી છે-શું તમે જાણો છો કે તે શું છે?', 'સડો વળાંકવા જેવો છે... કોને રોકવું?', 'જ્યારે વિલ ફેરેલ સફરજન વિશે મજાક કરી રહ્યો હતો ત્યારે શાકભાજી વિભાગમાં હાસ્યની લહેર હતી!', 'આંતરિક વ્યવસ્થાના નિરીક્ષકે જે રીતે વાંધો ઉઠાવ્યો હતો તે જ રીતે, કેટલીકવાર લગભગ માતાનો ભાગ બનવું, ખરેખર અગાઉ ક્યારેય ખોવાઈ ન ગયું હોવાનું સૂચવે છે.', '\"ધ હેન્ડમેઇડ્સ ટેલ\" નો નવો સીઝનઃ અતિવાદી નારીવાદના જોખમો પર ધ્યાન કેન્દ્રિત કરે છે.']\n"
     ]
    }
   ],
   "source": [
    "print(tl_to_gu_title[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zn0l6PVDTeTz"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Open a new text file for writing\n",
    "with open('tel_guj_non_test.csv', 'w', newline='', encoding='utf-8') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        # Write the header row\n",
    "        writer.writerow(['text', 'title'])\n",
    "        # Write the data from the lists\n",
    "        for text, title in zip(tl_to_gu_text, tl_to_gu_title):\n",
    "            writer.writerow([text, title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OXV1p4UXF3B"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def merge_csv(source_csv_path, target_csv_path):\n",
    "    # Lists to store text and title from the source CSV\n",
    "    text_list = []\n",
    "    title_list = []\n",
    "\n",
    "    # Read data from the source CSV and populate the lists\n",
    "    with open(source_csv_path, 'r', newline='', encoding='utf-8') as source_file:\n",
    "        reader = csv.DictReader(source_file)\n",
    "        for row in reader:\n",
    "            text_list.append(row['text'])\n",
    "            title_list.append(row['title'])\n",
    "\n",
    "    # Append the text and title to the target CSV\n",
    "    with open(target_csv_path, 'a', newline='', encoding='utf-8') as target_file:\n",
    "        writer = csv.DictWriter(target_file, fieldnames=['text', 'title'])\n",
    "        # Check if the target CSV is empty; if so, write the header row\n",
    "        if target_file.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        # Write the data from the source CSV to the target CSV\n",
    "        for text, title in zip(text_list, title_list):\n",
    "            writer.writerow({'text': text, 'title': title})\n",
    "\n",
    "# Example usage:\n",
    "merge_csv('tel_guj_non_test.csv','gujarati_non_sar_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tDEPI9qqRqE"
   },
   "source": [
    "# **Sarcasm Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW78FCcnqOMX",
    "outputId": "4591522c-cf3c-4b78-a9fa-0dd277535c2e"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MstlnU29JOt-"
   },
   "outputs": [],
   "source": [
    "l1=[]\n",
    "l2=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPFxXBQFqQ_O"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def classify_news_articles(articles):\n",
    "    print(\"started\")\n",
    "    sarcastic_articles = []\n",
    "    non_sarcastic_articles = []\n",
    "    i=1\n",
    "    for article in articles:\n",
    "        print(i)\n",
    "        i=i+1\n",
    "        sequence_to_classify = article['text']\n",
    "        candidate_labels = ['sarcastic', 'non-sarcastic']\n",
    "        result = classifier(sequence_to_classify, candidate_labels)\n",
    "\n",
    "        if result['scores'][0] > 0.57:  # If sarcastic score is greater than 0.5\n",
    "            sarcastic_articles.append({'text': article['text'], 'title': article['title']})\n",
    "            l1.append({'text': article['text'], 'title': article['title']})\n",
    "        else:\n",
    "            non_sarcastic_articles.append({'text': article['text'], 'title': article['title']})\n",
    "            l2.append({'text': article['text'], 'title': article['title']})\n",
    "\n",
    "    return sarcastic_articles, non_sarcastic_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jBU_1q-tMrA"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function to read data from CSV file and convert it into a list of dictionaries\n",
    "def read_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            data.append({'text': row['text'], 'title': row['title']})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eXcBbjXdcNs"
   },
   "source": [
    "# **Split-data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeXybyVfoaUu"
   },
   "source": [
    "## **Filter non-sarcastic Data to match sarcsatic data fields**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zf51WoU5fIpt"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def filter_csv(csv_file, filtered_csv_file, fields_to_keep):\n",
    "    # Read data from CSV file\n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        fieldnames = reader.fieldnames\n",
    "\n",
    "        # Filter out headers not in fields_to_keep\n",
    "        filtered_fieldnames = [field for field in fieldnames if field in fields_to_keep]\n",
    "\n",
    "        # Write filtered data to new CSV file\n",
    "        with open(filtered_csv_file, 'w', newline='', encoding='utf-8') as fw:\n",
    "            writer = csv.DictWriter(fw, fieldnames=filtered_fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in reader:\n",
    "                filtered_row = {field: row[field] for field in filtered_fieldnames}\n",
    "                writer.writerow(filtered_row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "sKjlvvFJfKaF",
    "outputId": "d88f8441-c403-40b9-d3bf-7cc68717a3df"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "csv_file = 'gujarati_nonsarcastic.csv'\n",
    "filtered_csv_file = 'gujarati_nonsarcastic_filter.csv'\n",
    "fields_to_keep = ['title', 'text']  # Specify the fields you want to keep\n",
    "filter_csv(csv_file, filtered_csv_file, fields_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXxFxcwWdgBF"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def split_csv(csv_file, train_csv_file, test_csv_file, train_samples=293, test_samples=70):\n",
    "    # Read data from CSV file\n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Randomly shuffle the data\n",
    "    # random.shuffle(data)\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    train_data = data[:train_samples]\n",
    "    test_data = data[train_samples:train_samples+test_samples]\n",
    "\n",
    "    # Write train data to CSV file\n",
    "    with open(train_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(train_data)\n",
    "\n",
    "    # Write test data to CSV file\n",
    "    with open(test_csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=reader.fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(test_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjRO6CM_ksK3"
   },
   "source": [
    "## **Split telugu data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-1H9FU3dlXp"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "csv_file = 'telugu_nonsarcastic_filter.csv'\n",
    "train_csv_file = 'telugu_non_sar_train.csv'\n",
    "test_csv_file = 'telugu_non_sar_test.csv'\n",
    "split_csv(csv_file, train_csv_file, test_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhbN_a-5eD4Q"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "csv_file = 'telugu_sarcastic.csv'\n",
    "train_csv_file = 'telugu_sar_train.csv'\n",
    "test_csv_file = 'telugu_sar_test.csv'\n",
    "split_csv(csv_file, train_csv_file, test_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE8QCggxlwiT"
   },
   "source": [
    "## **Split Gujarati Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GDDRmrsYl0QS"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "csv_file = 'gujarati_nonsarcastic.csv'\n",
    "train_csv_file = 'gujarati_non_sar_train.csv'\n",
    "test_csv_file = 'gujarati_non_sar_test.csv'\n",
    "split_csv(csv_file, train_csv_file, test_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggqhCpOAmXEu"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "csv_file = 'gujarati_sarcastic.csv'\n",
    "train_csv_file = 'gujarati_sar_train.csv'\n",
    "test_csv_file = 'gujarati_sar_test.csv'\n",
    "split_csv(csv_file, train_csv_file, test_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVkU8H9jfhqi"
   },
   "source": [
    "# **Combine 2 test csv files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ncg8XLfEfhX_"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def combine_csv(csv_file1, csv_file2, combined_csv_file):\n",
    "    # Read data from CSV file 1\n",
    "    with open(csv_file1, 'r', encoding='utf-8') as f1:\n",
    "        reader1 = csv.DictReader(f1)\n",
    "        fieldnames = reader1.fieldnames\n",
    "        data1 = list(reader1)\n",
    "\n",
    "    # Read data from CSV file 2\n",
    "    with open(csv_file2, 'r', encoding='utf-8') as f2:\n",
    "        reader2 = csv.DictReader(f2)\n",
    "        data2 = list(reader2)\n",
    "\n",
    "    # Combine data from both CSV files\n",
    "    combined_data = data1 + data2\n",
    "\n",
    "    # Write combined data to new CSV file\n",
    "    with open(combined_csv_file, 'w', newline='', encoding='utf-8') as fw:\n",
    "        writer = csv.DictWriter(fw, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(combined_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUHaPts4kwh1"
   },
   "source": [
    "## **Combine telugu csvs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSVxE9hTflpD"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "csv_file1 = 'telugu_sarcastic_test.csv'\n",
    "csv_file2 = 'telugu_non_sar_test.csv'\n",
    "combined_csv_file = 'telugu_test.csv'\n",
    "combine_csv(csv_file1, csv_file2, combined_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1mDLnm_mJuW"
   },
   "source": [
    "## **Combine Gujarati csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J8EAlUImIzq"
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "csv_file1 = 'gujarati_sar_test_m.csv'\n",
    "csv_file2 = 'gujarati_non_sar_test_m.csv'\n",
    "combined_csv_file = 'gujarati_test.csv'\n",
    "combine_csv(csv_file1, csv_file2, combined_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BcfSawmc6Gm"
   },
   "source": [
    "# **1) Telugu Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14zUh-IBtOw5"
   },
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "file_path = 'telugu_test.csv'  # Replace 'test.csv' with the path to your CSV file\n",
    "articles = read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1Hj1G7qrtCB",
    "outputId": "69920f01-4638-4ed9-a355-635eaa13fe9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShLaITIbrvza",
    "outputId": "663adcb6-0d3b-4944-e143-d6d9f6585e7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uysexjUKqJOg",
    "outputId": "f8f74a58-334e-495e-a00a-fac53d524d9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic articles saved to: tl_sarcastic_articles_test.csv\n",
      "Non-Sarcastic articles saved to: tl_non_sarcastic_articles_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to write data to CSV file\n",
    "def write_csv(file_path, data):\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['text', 'title'])\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Write sarcastic articles to CSV file\n",
    "sarcastic_file_path = 'tl_sarcastic_articles_test.csv'\n",
    "write_csv(sarcastic_file_path, l1)\n",
    "\n",
    "# Write non-sarcastic articles to CSV file\n",
    "non_sarcastic_file_path = 'tl_non_sarcastic_articles_test.csv'\n",
    "write_csv(non_sarcastic_file_path, l2)\n",
    "\n",
    "print(\"Sarcastic articles saved to:\", sarcastic_file_path)\n",
    "print(\"Non-Sarcastic articles saved to:\", non_sarcastic_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LB9lmespb9v"
   },
   "source": [
    "# **1) Gujarati Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUjcnP2rpb95"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# Load data from CSV file\n",
    "file_path = 'gujarati_test.csv'  # Replace 'test.csv' with the path to your CSV file\n",
    "articles = read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3_v7pzlMLXP",
    "outputId": "0dc16787-83c0-4c73-9fe5-f39fb4cbce13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVLSYH_ofiC4",
    "outputId": "8211c019-e16d-4f9c-ccbe-0d11455b3c78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'અબુ ધાબીમાં અંડરવર્લ્ડ ડોન દાઉદ ઇબ્રાહિમના શાર્પ શૂટર રાશીદ માલબારીની ધરપકડ પછી ચોંકાવનારો ખુલાસો થયો છે. રાશીદ માલબારીએ છોટા શકીલના કહેવાથી શ્રીરામ સેનાના સંસ્થાપક પ્રમોદ મુથાલિક અને બીજેપી નેતા વરૂણ ગાંધીને મારવાનો પ્લાન બનાવ્યો હતો પરંતુ આ ષડયંત્ર પુરૂં થાય તે પહેલા જ તેના શૂટરની ધરપકડ કરી લેવામાં આવી હતી.\\nરાશીદ વર્ષ 2014માં મેંગલુરૂ કોર્ટમાંથી નેપાળના રસ્તે ભારતમાંથી ફરાર થઇ ગયો હતો. અંડરવર્લ્ડનું નેપાળનું બધું કામ રાશીદ જ સંભાળે છે. બેંગકોકમાં વર્ષ 2000માં છોટા રાજન પર હુમલામાં રાશીદ પણ સામેલ હતો. હુમલામાં છોટા રાજનને ગોળી વાગી હતી પરંતુ તે ફરાર થઇ ગયો હતો. આ હુમલામાં છોટા રાજનનો નજીકનો માણસ રોહીત વર્મા માર્યો ગયો હતો. તે સમયે રાશીદે છોટા રાજન પર પણ ગોળી મારી હતી. તેની પર હત્યાના ઘણાં કેસ નોંધાયા છે. મેંગલુરૂ કોર્ટમાંથી ફરાર થયા બાદ પોલીસે તેની સામે લુકઆઉટ નોટિસ જાહેર કરી હતી. તેની સામે રેડ કોર્નર નોટિસ પણ જાહેર થઇ ચુકી છે.\\nરાશીદ ડી ગેંગનો ભારતનો સૌથી મોટો માણસ માનવામાં આવે છે. તેણે છોટા રાજન પર હુમલા ઉપરાંત ક્વાલાલમ્પુરમાં છોટા રાજનના નજીકના માણસની હત્યામાં પણ તેનો હાથ હતો. સુરક્ષા એજન્સિઓ ધરપકડ પછી રાશીદને ભારત લાવવાનો પ્રયત્ન કરી રહી હતી. રાશીદની ધરપકડની પુષ્ટિ છોટા શકીલે પણ કરી છે.',\n",
       "  'title': 'વરૂણ ગાંધીને મારવા માંગતો હતો દાઉદનો આ શૂટર, અબુ ધાબીથી ઝડપાયો'},\n",
       " {'text': '\\nઔ। દુબઇ\\xa0 ।\\nભારતીય ક્રિકેટ ટીમ આઇસીસીની બીજી વર્લ્ડ ટેસ્ટ ચેમ્પિયનશિપની સિઝનમાં વધુ એક વખત પોતાની પરંપરાગત હરીફ પાકિસ્તાન સામે રમશે નહીં. ભારતીય ટીમે આગામી બે વર્ષમાં ત્રણ વિદેશ પ્રવાસ ખેડવાના છે જેની શરૃઆત આગામી મહિને ઇંગ્લેન્ડ સામેની પાંચ ટેસ્ટની શ્રેણી સાથે થશે. આઇસીસીના નિયમો અનુસાર વર્લ્ડ ટેસ્ટ ચેમ્પિયનશિપના કેલેન્ડરમાં નવ દેશોએ પોતાની પસંદગીના ૬ દેશ સામે રમવાનું છે અને ૨૦૨૧-૨૩ના કેલેન્ડરમાં ભારતીય ટીમ વેસ્ટ ઇન્ડિઝના બદલે શ્રીલંકા સામે રમશે.\\nભારતે ૨૦૧૭ના વર્લ્ડ ટેસ્ટ ચેમ્પિયનશિપના પ્રારંભિક તબક્કામાં વેસ્ટ ઇન્ડિઝ સામે શ્રેણી રમી હતી. ભારતીય ટીમ ૨૦૨૧ના ડિસેમ્બરથી ૨૦૨૨ના જાન્યુઆરીની વચ્ચે સાઉથ આફ્રિકાનો પ્રવાસ ખેડયા બાદ ૨૦૨૨ના નવેમ્બરમાં બાંગ્લાદેશ સામે તેની જ ધરતી ઉપર રમશે.\\nઆગામી વર્ષના સપ્ટેમ્બર તથા નવેમ્બરમાં ભારત પોતાના ઘરઆંગણે ઓસ્ટ્રેલિયા સામેની શ્રેણી રમશે. તે પહેલાં ભારત બે ટેસ્ટ મેચ માટે ન્યૂઝીલેન્ડની અને બાંગ્લાદેશની યજમાની કરશે. આઇસીસીએ બુધવારે જાહેરાત કરી હતી કે ડબ્લ્યૂટીસીની બીજી સિઝનમાં એક ટેસ્ટ જીતવા બદલ ૧૨ પોઇન્ટ આપવામાં આવશે. ડ્રો માટે ચાર તથા ટાઇ માટે ૬ પોઇન્ટ આપવામાં આવશે.\\n',\n",
       "  'title': 'WTCની બીજી સિઝનમાં પણ ભારત અને પાકિસ્તાન આમનેસામને નહીં થાય'},\n",
       " {'text': 'નવી દિલ્હી, તા. 9. ઓક્ટોબર 2020 શુક્રવાર\\nઉત્તર કોરિયાના ભેજાગેપ અને ક્રુર તાનાશાહ કિમ જોંગ ઉનની ક્રુરતાની વધુ એક કહાની બહાર આવી છે.\\nઉત્તર કોરિયાની જેલમાંથી ભાગીને ગમે-તેમ કરીને અમેરિકા પહોંચી ગયેલા એક કેદીએ એક માનવાધિકાર સંસ્થાને વોશિંગ્ટનમાં આપેલા ઈન્ટરવ્યૂમાં ચોંકાવનારો ખુલાસો કરતા કહ્યુ હતુ કે, ઉત્તર કોરિયામાં વિદેશી ટીવી શો જોવાની ભયાનક સજા અપાય છે.આવા શો જોનારા કેદીઓે જેલમાં તેમના મૃત સાથીદારના સળગાવી દેવાયેલા મૃતદેહની રાખ સાથે ભળેલુ પાણી પીવાની પરજ પડાય છે.\\nચોંચરી નામની જેલમાં કેદીયો સાથે જાનવરો કરતા પણ ખરાબ વર્તન કરવામાં આવે છે તેવો ધડાકો કરનાર આ કેદીએ ઉમેર્યુ હતુ કે, મૃત કેદીઓના શરીરને સળગાવી દેવાતા પહેલા એક ગોડાઉનમાં રાખવામાં આવે છે જ્યાં ઉંદરો મૃતદેહની ઉજવાણી પણ કરે છે.\\nવિદેશી ટીવી શો જોતા પકડાયેલા લોકો માટે આ જેલ બનાવવામાં આવી છે.જ્યાં કેદીઓને અમાનુષી યાતના અપાય છે.દર સપ્તાહે અહીંયા કોઈને કોઈ કેદીનુ મોત થયા છે.જેને જેલમાં જ બનેલા એક શબગૃહમાં સળગાવી દેવાય છે.કેમ્પમાં દર સોમવારે મરી ગયેલા કેદીઓને અગ્નિદાહ અપાય છે.સળગેલા મૃતદેહોની રાખનો ખાતર તરીકે ઉપયોગ કરવા માટે ખુલ્લામાં તેનો ઢગલો કરાતો હોય છે.જ્યારે પણ વરસાદ પડે ત્યારે આ રાખ પાણીની સાથે વહીને નજીકની નદીમાં પહોંચી જતી હોય છે અને કેદીઓને આ નદીનુ પાણી નહાવા માટે અને પીવા માટે અપાય છે.\\nઆ ઈન્ટરવ્યૂ આપનાર કેદીનુ નામ અને ઓળખ સંસ્થાએ સુરક્ષાના કારણોસર ગુપ્ત રાખી છે.સંસ્થાનુ કહેવુ છે કે, કેદી જે જેલની વાત કરે છે તેની ઓળખ સેટેલાઈટ ઈમેજની મદદથી થઈ શકી છે.\\n',\n",
       "  'title': 'કિમ જોંગનો અત્યાચાર, વિદેશી ટીવી શો જોતા પકડાયેલાને મૃતદેહોની રાખવાળુ પાણી પિવડાવાય છે'},\n",
       " {'text': '\\nમુંબઈ, તા. ૨૦\\nઆવતા અઠવાડિયા સુધીમાં પ્રધાનમંડળનો વિસ્તાર થવાની સંભાવના હોવાથી એમાં મુંબઈ ભાજપના અધ્યક્ષ આશિષ શેલાર અને રાષ્ટ્રવાદી કોન્ગ્રેસમાંથી ભાજપમાં આવેલા નેતા વિજયકુમાર ગાવિતને પણ તક મળે એવી ચર્ચા રાજકીય વર્તુળોમાં થઈ રહી છે.\\nવિજયકુમાર ગાવિત કોન્ગ્રેસ-રાષ્ટ્રવાદીની સરકાર વખતે આદિવાસી વિકાસ પ્રધાન હતા. એ વખતે તેમણે વિદ્યાર્થીઓ માટેના સાહિત્યમાં ગોટાળો કર્યો હોવાનો આરોપ ખુદ ભાજપના નેતાઓએ મૂક્યો હતો. આરોપીઓને ચૂંટણીમાં ટિકિટ આપી પવિત્ર કરવાની રીત કેબિનેટ વિસ્તરણમાં પણ કાયમ રહેવાની શક્યતા છે.\\nલોકસભાની ચૂંટણી માથે હતી ત્યારે જ વિજયકુમાર ગાવિતે ભાજપમાં પ્રવેશ કર્યો અને દીકરી હિના ગાવિતને પણ નંદુરબારમાંથી લોકસભામાં મોકલાવી હતી.\\nસત્તામાં આવ્યા બાદ અમારે ઘડાવાનું છે એ માટે વિજયકુમાર ગાવિત અને બબનરાવ પાચપુતેને ભાજપમાં પ્રવેશ આપ્યો છે, એવો પાંગળો બચાવ ભાજપના તત્કાલીન પ્રદેશાધ્યક્ષ દેવેન્દ્ર ફડણવીસે ૨૯ સપ્ટેમ્બર, ૨૦૧૪ના રોજ કર્યો હતો.\\nવિજયકુમાર ગાવિત ઉપર ગોટાળાનો આરોપ અમે મૂક્યો નથી. પાચપુતે પર મૂક્યો હતો. અમારી સરકાર આવી છતાં તપાસ ચાલુ રહેશે. એમાં તે દોષી સાબિત થયા તો તેમના પર કાર્યવાહી કરવામાં આવશે, એવું એ સમયે ફડણવીસે કહ્યું હતું.\\nઆ અગાઉ ૮ જુલાઈ, ૨૦૧૬ના રોજ રાજ્યના પ્રધાનમંડળનો વિકાસ કરવામાં આવ્યો હતો. રાજ્યપાલ સી. વિદ્યાસાગર રાવે ભાજપના સાત, શિવસેનાના બે અને મિત્ર પક્ષોના બે જણને પદ અને ગોપનીયતાના સોગંદ લેવડાવ્યા હતા.\\n',\n",
       "  'title': 'મહારાષ્ટ્રના પ્રધાનમંડળ વિસ્તરણમાં શેલાર-ગાવિતના સમાવેશના ભણકારા'},\n",
       " {'text': 'ઊંઝા,તા.12 ફેબ્રુઆરી 2021, શુક્રવાર\\nઊંઝા નગરપાલિકામાં આજે ચુંટણી ફોર્મ ભરવા સમર્થકો સાથે આવતા ઉમેદવારોનો રાફડો ફાટતાં નગરપાલિકા કમ્પાઉન્ડમાં ભારે ભીડ થતાં જાણે કોરોના સંપૂર્ણ ચાલ્યો ગયો હોય તેવા દ્રશ્યો જોવા મળ્યા હતા. કોરોનાની સરકારી ગાઈડલાઈનના આજે પાલિકા કમ્પાઉન્ડમાં લીરેલીરા ઉડયા હતા.\\nકોરોનાની જાહેર ગાઈડ લાઈન હેઠળ માસ્ક પહેર્યું નહી હોય તો પણ હજાર રૂપિયાનો દંડ ફટકારતી પોલીસ આજે મૂકપ્રેક્ષક બની ગઈ હતી. ઊંઝા નગરપાલિકામાં ચુંટણીમાં આજે ૮૦ જેટલા ફોર્મ ભરાયા હતા. આજે સવારથી ઉમેદવારો પોતાના ટેકેદારો સાથે સમુહમાં રેલીઓ કાઢીને આવતાં નગરપાલિકામાં ભારે ભીડ થઈ હતી. નગરપાલિકાની ચુંટણીમાં ૩૬ બેઠકો ઉપર ૩૦૦ કરતાં પણ વધારે ફોર્મનું વિતરણ થયા બાદ અત્યારસુધીમાં ૧૦૦ ઉપરાંત ઉમેદવારી ફોર્મ ભરાયા હતા. જેના લીધે પાલિકામાં ભાજપની સામે અપક્ષ મોરચો મેદાને પડયો છે. આજે ભાજપના મેન્ડેટ ધરાવતા તમામ ઉમેદવારોએ ચુંટણી ફોર્મ ભરીને કાર્યવાહી પૂર્ણ કરી છે. જ્યારે મેન્ટેડ નહિ મળતા નારાજ થયેલા ઉમેદવારોએ અપક્ષમાં ઉમેદવારી કરી ભાજપા સામે બાંયો ચડાવતાં નગરમાં ચુંટણી જંગ જામ્યો છે.\\nબીજી તરફ ઊંઝા તાલુકા પંચાયત તથા જિલ્લા પંચાયતની બેઠકો ઉપર ખુબ મોળો પ્રતિસાદ સોંપડી રહ્યો છે. ઊંઝા તાલુકા પંચાયતની ૧૮ બેઠકો ઉપર ૪૦ ફોર્મ ભરાયા છે. જ્યારે જિલ્લા પંચાયતની ત્રણ બેઠકો ઉપર અત્યાર સુધીમાં ૧૦ ફોર્મ ભરાયા છે. ઊંઝા તાલુકા પંચાયતની ૧૮ બેઠકોના ભાજપે જાહેર કરેલી યાદીમાં પાટીદારોનો દબદબો વધુ રહ્યો છે. જોકે તાલુકા-જિલ્લા પંચાયતની બેઠકોની ચુંટણીમાં હજુ મંદિનો માહોલ રહ્યો છે. જેમાં ઉનાવા-૨ની બેઠક ઉપર હજુ એકપણ ફોર્મ ભરાયું છે. જોકે હજુ ફોર્મ પરત ખેંચાયા બાદ ચિત્ર સ્પષ્ટ થાય તેમ છે. જોકે ગ્રામ્ય લેવલ કરતાં શહેરી વિસ્તારમાં કહી ખુશી કહી ગામનો માહોલ વધુ જોવા મળી રહ્યો છે. આવતીકાલે શનિવારે ફોર્મ ભરવાનો છેલ્લો દિવસ બાદ ઊંઝા પાલિકાની ચુંટણી મેદાનમાં કેટલા ઉમેદવારોએ ઝંપલાવ્યું છે તે ચિત્ર સ્પષ્ટ થાય તેમ છે.\\n',\n",
       "  'title': 'ઊંઝા નગરપાલિકામાં ચુંટણી ફોર્મ ભરવા સમર્થકો સાથે ઉમેદવારોનો રાફડો ફાટયો'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59AEyqRcpb95",
    "outputId": "5639ef35-11ef-4122-b71c-0cbff829c8c6"
   },
   "outputs": [],
   "source": [
    "# Call the classify_news_articles function\n",
    "sarcastic_articles, non_sarcastic_articles = classify_news_articles(articles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ghe-fHj8pb95",
    "outputId": "e6055c83-3124-4d82-d8e7-05ce810e7665"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTgrhb69pb95",
    "outputId": "6149521f-2e28-43b3-e22e-8c934d2b6aa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJIq3agCpb95",
    "outputId": "5ec7407c-9d23-496a-8b92-13a1935bd2ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcastic articles saved to: gu_sarcastic_articles_test.csv\n",
      "Non-Sarcastic articles saved to: gu_non_sarcastic_articles_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to write data to CSV file\n",
    "def write_csv(file_path, data):\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['text', 'title'])\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Write sarcastic articles to CSV file\n",
    "sarcastic_file_path = 'gu_sarcastic_articles_test.csv'\n",
    "write_csv(sarcastic_file_path, l1)\n",
    "\n",
    "# Write non-sarcastic articles to CSV file\n",
    "non_sarcastic_file_path = 'gu_non_sarcastic_articles_test.csv'\n",
    "write_csv(non_sarcastic_file_path, l2)\n",
    "\n",
    "print(\"Sarcastic articles saved to:\", sarcastic_file_path)\n",
    "print(\"Non-Sarcastic articles saved to:\", non_sarcastic_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBGQlMq8q3V5"
   },
   "source": [
    "# **Headline Genration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GIocLFnDK_Kp",
    "outputId": "b9405f52-37cc-422e-cf0d-e20f434aece5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.12.0 (from -r requirements.txt (line 1))\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=1.8.0 (from -r requirements.txt (line 2))\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.1.99)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (3.20.3)\n",
      "Collecting rouge-score (from -r requirements.txt (line 5))\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.8.1)\n",
      "Collecting py7zr (from -r requirements.txt (line 7))\n",
      "  Downloading py7zr-0.21.0-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (2.2.1+cu121)\n",
      "Collecting evaluate (from -r requirements.txt (line 9))\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (24.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.13.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.66.2)\n",
      "Collecting xxhash (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets>=1.8.0->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.9.5)\n",
      "Collecting huggingface-hub (from accelerate>=0.12.0->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 6)) (2023.12.25)\n",
      "Collecting texttable (from py7zr->-r requirements.txt (line 7))\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pycryptodomex>=3.16.0 (from py7zr->-r requirements.txt (line 7))\n",
      "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyzstd>=0.15.9 (from py7zr->-r requirements.txt (line 7))\n",
      "  Downloading pyzstd-0.15.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (411 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.2/411.2 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr->-r requirements.txt (line 7))\n",
      "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pybcj<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 7))\n",
      "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr->-r requirements.txt (line 7))\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->-r requirements.txt (line 7))\n",
      "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting brotli>=1.1.0 (from py7zr->-r requirements.txt (line 7))\n",
      "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (3.1.3)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->-r requirements.txt (line 8)) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->-r requirements.txt (line 8))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Collecting responses<0.19 (from evaluate->-r requirements.txt (line 9))\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->-r requirements.txt (line 8)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->-r requirements.txt (line 8)) (1.3.0)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=0d645680116434b2e8786bef804511cb9dee14466123a774616e840295522387\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: texttable, brotli, xxhash, pyzstd, pyppmd, pycryptodomex, pybcj, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multivolumefile, inflate64, dill, rouge-score, responses, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, nvidia-cusolver-cu12, datasets, evaluate, accelerate\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed accelerate-0.29.3 brotli-1.1.0 datasets-2.19.0 dill-0.3.8 evaluate-0.4.1 huggingface-hub-0.22.2 inflate64-1.0.0 multiprocess-0.70.16 multivolumefile-0.2.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 py7zr-0.21.0 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.15.10 responses-0.18.0 rouge-score-0.1.2 texttable-1.7.0 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JEoSf7iTK1Gw",
    "outputId": "47dbdc39-8f5f-4b08-e076-167338d30891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 197124, done.\u001b[K\n",
      "remote: Counting objects: 100% (602/602), done.\u001b[K\n",
      "remote: Compressing objects: 100% (276/276), done.\u001b[K\n",
      "remote: Total 197124 (delta 331), reused 502 (delta 266), pack-reused 196522\u001b[K\n",
      "Receiving objects: 100% (197124/197124), 209.22 MiB | 24.48 MiB/s, done.\n",
      "Resolving deltas: 100% (140167/140167), done.\n",
      "Updating files: 100% (4243/4243), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNSJ7vtfK5ik",
    "outputId": "8967ec0f-1af2-4deb-fa0b-bb3273e1e665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ./transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scKUMICAqQLO",
    "outputId": "f603a168-c21b-4f79-ad8e-20c24f4d7d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ENGZqy9D7my"
   },
   "source": [
    "## **Gujarati sarcastic model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dex6I5-NLo-c",
    "outputId": "01c4ecfb-7fab-4f72-8751-1ff89d08048d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:31:29.015700: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-22 15:31:29.015765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-22 15:31:29.017749: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-22 15:31:31.077593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using custom data configuration default-b72e0611477824f5\n",
      "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
      "Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-b72e0611477824f5/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5)\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-b72e0611477824f5/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 360 examples [00:00, 11222.13 examples/s]\n",
      "Generating test split\n",
      "Generating test split: 20 examples [00:00, 4991.14 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-b72e0611477824f5/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:726] 2024-04-22 15:31:39,975 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-22 15:31:39,979 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:726] 2024-04-22 15:31:40,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-22 15:31:40,058 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-22 15:31:40,059 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-22 15:31:40,059 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-22 15:31:40,059 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-22 15:31:40,059 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-22 15:31:40,059 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:726] 2024-04-22 15:31:40,059 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-22 15:31:40,060 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:329] 2024-04-22 15:31:40,589 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "[INFO|configuration_utils.py:726] 2024-04-22 15:31:41,579 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-22 15:31:41,580 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:3430] 2024-04-22 15:31:42,525 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:936] 2024-04-22 15:31:42,573 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4171] 2024-04-22 15:31:46,214 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4179] 2024-04-22 15:31:46,214 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:891] 2024-04-22 15:31:46,265 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/generation_config.json\n",
      "[INFO|configuration_utils.py:936] 2024-04-22 15:31:46,265 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Running tokenizer on train dataset:   0% 0/360 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b72e0611477824f5/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5/cache-304fb24d518382a1.arrow\n",
      "Running tokenizer on train dataset: 100% 360/360 [00:00<00:00, 882.54 examples/s]\n",
      "Running tokenizer on prediction dataset:   0% 0/20 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b72e0611477824f5/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5/cache-1dbd68d9b67898a4.arrow\n",
      "Running tokenizer on prediction dataset: 100% 20/20 [00:00<00:00, 497.92 examples/s]\n",
      "[INFO|trainer.py:2048] 2024-04-22 15:31:48,247 >> ***** Running training *****\n",
      "[INFO|trainer.py:2049] 2024-04-22 15:31:48,247 >>   Num examples = 360\n",
      "[INFO|trainer.py:2050] 2024-04-22 15:31:48,247 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2051] 2024-04-22 15:31:48,247 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2054] 2024-04-22 15:31:48,247 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2055] 2024-04-22 15:31:48,247 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2056] 2024-04-22 15:31:48,248 >>   Total optimization steps = 450\n",
      "[INFO|trainer.py:2057] 2024-04-22 15:31:48,249 >>   Number of trainable parameters = 300,176,768\n",
      "100% 450/450 [02:57<00:00,  2.40it/s][INFO|trainer.py:2316] 2024-04-22 15:34:45,488 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100% 450/450 [02:57<00:00,  2.54it/s]\n",
      "[INFO|trainer.py:3305] 2024-04-22 15:34:45,490 >> Saving model checkpoint to output_mt5_gu/\n",
      "[INFO|configuration_utils.py:471] 2024-04-22 15:34:45,493 >> Configuration saved in output_mt5_gu/config.json\n",
      "[INFO|configuration_utils.py:705] 2024-04-22 15:34:45,493 >> Configuration saved in output_mt5_gu/generation_config.json\n",
      "[INFO|modeling_utils.py:2591] 2024-04-22 15:34:59,215 >> Model weights saved in output_mt5_gu/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2489] 2024-04-22 15:34:59,216 >> tokenizer config file saved in output_mt5_gu/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2498] 2024-04-22 15:34:59,217 >> Special tokens file saved in output_mt5_gu/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-04-22 15:34:59,225 >> Copy vocab file to output_mt5_gu/spiece.model\n",
      "[INFO|trainer.py:3614] 2024-04-22 15:34:59,330 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3616] 2024-04-22 15:34:59,330 >>   Num examples = 20\n",
      "[INFO|trainer.py:3619] 2024-04-22 15:34:59,330 >>   Batch size = 8\n",
      "100% 3/3 [00:02<00:00,  1.21it/s]\n",
      "[INFO|modelcard.py:450] 2024-04-22 15:35:08,136 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path google/mt5-small \\\n",
    "    --do_train True \\\n",
    "    --do_eval False \\\n",
    "    --do_predict True \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --train_file gujarati_sar_train.csv \\\n",
    "    --test_file gu_sarcastic_articles_test.csv \\\n",
    "    --text_column \"text\" \\\n",
    "    --summary_column \"title\" \\\n",
    "    --max_target_length 298 \\\n",
    "    --output_dir output_mt5_gu/ \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --logging_strategy \"epoch\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --overwrite_output_dir True \\\n",
    "    --predict_with_generate $@ 2>&1>./hg_mt5_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ArRmV1B2ZCJL",
    "outputId": "ccb08f28-b233-402e-8cb4-c8a331eacdd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hYRIBlm5wB5"
   },
   "outputs": [],
   "source": [
    "!cp -r gu_sarcastic_articles_test.csv /content/drive/My\\ Drive/saved_trainer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONAMUFgnZJmA"
   },
   "outputs": [],
   "source": [
    "!cp -r output_mt5_gu /content/drive/My\\ Drive/saved_trainer/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGcXdIYqt2Pv",
    "outputId": "3a47133e-5fcc-468b-d739-6db2d7d63ef4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1155: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <extra_id_0> ભારતમાં થયેલા હુમલામાં પાકિસ્તાનને ભારતને\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"output_mt5_gu\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"output_mt5_gu\")\n",
    "\n",
    "# Define input text\n",
    "input_text = \"પુલવામાં હુમલા પર મંગળવારે પાકિસ્તાની પીએમ ઇમરાન ખાને ભારતને એડ્રેસ કર્યું. તેની પર પ્રખ્યાત રાઇટર જાવેદ અખ્તરે જવાબ આપ્યો છે. સાથે જ હુમલાની જવાબદારી ન લેવા પર પાકિસ્તાનની આલોચના કરી છે.\\nએટલું જ નહીં, અખ્તરે આ ઘટનાને પણ શેર કરી છે જેમા તેમને પાકિસ્તાની ન્યૂઝ એન્કર પર નિશાન સાધ્યું હતું. અખ્તરે કહ્યું, ઇમરાને નો બોલ ફેંક્યો છે. દર વખત તે પુછે છે કે તમને કેમ લાગે છે કે આ અમે કર્યું છે.\\n<>\\n\\nજાવેદ અખ્તરે એક ઘટના અંગે જણાવતા લખ્યું, મુંબઇ આતંકી હુમલા બાદ પાકિસ્તાનની એક ટીવી એન્કરે મને પૂછ્યું કે શુ તમે એવું સમજો છો કે આ પાકિસ્તાને કર્યું છે. આ તો કોઇપણ દેશ હોય શકે છે. મેં પણ કહ્યું કે ઠીક છે ચલો તમને 3 ઓપ્શન આપીશ. તમારે એકને પસંદ કરવાનું છે. બ્રાઝીલ, સ્વીડન અને પાકિસ્તાન.\\nજણાવી દઇએ કે પાક. પીએમ ઇમરાન ખાને આશરે 6 મિનિટના લાંબા વીડિયોમાં આ વાતથી ઇન્કાર કર્યો છે કે ભારતમાં થયેલા હુમલામાં પાકિસ્તાનનો હાથ છે. જો પાકિસ્તાને હુમલો કર્યો છે તો તેના પુરાવા આપે. ઇમરાને ભારતને ધમકી પણ આપી હતી કે ભારત પાકિસ્તાન પર અટેક કરશે તો તે પણ તેનો જડબાતોડ જવાબ આપશે.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode output tokens\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print output text\n",
    "print(\"Output:\", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQ8BaG00wsQe"
   },
   "source": [
    "## **Gujarati non sarcastic model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "300IzORVwqSw"
   },
   "outputs": [],
   "source": [
    "!python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path google/mt5-small \\\n",
    "    --do_train True \\\n",
    "    --do_eval False \\\n",
    "    --do_predict True \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --train_file gujarati_non_sar_train_m.csv \\\n",
    "    --test_file gujarati_non_sarcastic_test_zeroM.csv \\\n",
    "    --text_column \"text\" \\\n",
    "    --summary_column \"title\" \\\n",
    "    --max_target_length 298 \\\n",
    "    --output_dir output_mt5_gu_non_sar_m/ \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --logging_strategy \"epoch\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --overwrite_output_dir True \\\n",
    "    --predict_with_generate $@ 2>&1>./hg_mt5_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJr4W78fw1-H"
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0LtI8ZYw4oZ"
   },
   "outputs": [],
   "source": [
    "!cp -r output_mt5_gu_non_sar_m /content/drive/My\\ Drive/saved_trainer/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4ZLtg8NxCF2"
   },
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"output_mt5_gu_non_sar_m\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"output_mt5_gu_non_sar_m\")\n",
    "\n",
    "# Define input text\n",
    "input_text = \"કોરોના મહામારી સંકટ વચ્ચે દેશના લાખો કેન્દ્રીય કર્મચારીઓને કેન્દ્ર સરકારે ખુશખબર આપ્યા છે. સરકારે કર્મચારીઓને મળનારા વેરિએબલ મોંઘવારી ભથ્થામાં વધારાની જાહેરાત કરી છે. સરકારની આ જાહેરાતથી લગભગ દોઢ કરોડ કેન્દ્રીય કર્મચારીઓના ચહેરા પર હાસ્ય જોવા મળશે. તેનો ફાયદો કોન્ટ્રાક્ટ પર કામ કરતા કર્મચારીઓને પણ થશે. કર્મચારીઓના પગાર ઉપરાંત તેમના પ્રોવિડન્ડ ફંડ અને ગ્રેચ્યુઈટી ઉપર પણ આ નિર્ણયની અસર જોવા મળશે.\\nબમણું થયું વેરિએબલ મોંઘવારી ભથ્થું\\n\\nકેન્દ્રીય શ્રમ અને રોજગાર મંત્રાલયે કેન્દ્રીય કર્મચારીઓ માટે વેરિએબલ મોંઘવારી ભથ્થામાં વધારો કર્યો છે. કર્મચારીઓને વેરિએબલ મોંઘવારી ભથ્થું હે પહેલા 105 રૂપિયા મહિના પ્રમાણે મળતું હતું તે હવે વધીને બમણું થયું છે. એટલે કે હવે 210 રૂપિયા દર મહિને મળશે.\\nકેન્દ્રીય શ્રમ અને રોજગાર મંત્રાલયના આ નિર્ણયથી કેન્દ્ર સરકાર, રેલવે, ખાણ, ઓઈલ ફિલ્ડ્સ, પોર્ટ અને કેન્દ્ર સરકાર સંલગ્ન અન્ય કાર્યાલયોમાં કામ કરનારા લગભગ 1.5 કરોડ કર્મચારીઓને તેનો સીધો ફાયદો મળે તેવી આશા છે. કેન્દ્ર સરકારના જણાવ્યાં મુજબ વેરિએબલ મોંઘવારી ભથ્થામાં વધારાનો ફાયદો કોન્ટ્રાક્ટ અને હંગામી રીતે કાર્યરત કર્મચારીઓને પણ મળશે.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode output tokens\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print output text\n",
    "print(\"Output:\", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaYqjz2KxHnO"
   },
   "source": [
    "# **Predict sarcastic data using sarcastic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SocecRevxaSC"
   },
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"output_mt5_gu_sar_m\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"output_mt5_gu_sar_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOLk1G_Yxe4S"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = \"gujarati_sarcastic_test_zeroM.csv\"  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract the \"title\" column\n",
    "titles = df[\"title\"]\n",
    "text = df[\"text\"]\n",
    "\n",
    "# Save the titles to a text file\n",
    "output_txt_file_path = \"ground_truth_gu_sar_title.txt\"  # Replace with the path to your output text file\n",
    "with open(output_txt_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for title in titles:\n",
    "        output_file.write(title + \"\\n\")\n",
    "\n",
    "output_txt_file_path = \"ground_truth_gu_sar_text.txt\"  # Replace with the path to your output text file\n",
    "with open(output_txt_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for title in titles:\n",
    "        output_file.write(title + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJFPIHs3xmg1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file.readlines()]\n",
    "    return sentences\n",
    "\n",
    "title_file = 'ground_truth_gu_sar_title.txt'\n",
    "text_file = 'ground_truth_gu_sar_text.txt'\n",
    "\n",
    "# Read sentences from files\n",
    "title_sentences = read_sentences_from_file(title_file)\n",
    "text_sentences = read_sentences_from_file(text_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYXpMLm7xpUs"
   },
   "outputs": [],
   "source": [
    "# Define input text as an array of sentences\n",
    "# input_texts = [\"કોરોના મહામારી સંકટ\", \"મહામારી\"]\n",
    "\n",
    "# List to store generated outputs\n",
    "output_texts = []\n",
    "\n",
    "# Process each sentence to generate output\n",
    "for input_text in text_sentences:\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # Decode output tokens\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Append output to the list\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "    # Print output text\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Output:\", output_text)\n",
    "    print()\n",
    "\n",
    "# Write output texts to a text file\n",
    "with open(\"generated_outputs.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for output_text in output_texts:\n",
    "        file.write(output_text + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7nfJxRVxsid"
   },
   "source": [
    "# **Calculate scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lysRiPHfxwmz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = \"gujarati_sarcastic_test_zeroM.csv\"  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract the \"title\" column\n",
    "titles = df[\"title\"]\n",
    "\n",
    "# Save the titles to a text file\n",
    "output_txt_file_path = \"ground_truth_gu_sar.txt\"  # Replace with the path to your output text file\n",
    "with open(output_txt_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for title in titles:\n",
    "        output_file.write(title + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6NNeXTexyLM"
   },
   "outputs": [],
   "source": [
    "!pip install rouge-score\n",
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jhs1SBckx14o"
   },
   "outputs": [],
   "source": [
    "# Open the text file for reading\n",
    "with open('generated_outputs.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read all lines from the file\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Modify each line to remove '<extra_id_0>'\n",
    "modified_lines = [line.replace('<extra_id_0> ', '') for line in lines]\n",
    "\n",
    "# Open the same text file for writing\n",
    "with open('generated_outputs.txt', 'w', encoding='utf-8') as file:\n",
    "    # Write the modified lines back to the file\n",
    "    file.writelines(modified_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2l1MI1Ztx4jT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file.readlines()]\n",
    "    return sentences\n",
    "\n",
    "generated_file = 'generated_outputs.txt'\n",
    "reference_file = 'ground_truth_gu_sar.txt'\n",
    "\n",
    "# Read sentences from files\n",
    "generated_sentences = read_sentences_from_file(generated_file)\n",
    "reference_sentences = read_sentences_from_file(reference_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udd29k9Jx7oR"
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBzp2bddx-YW"
   },
   "outputs": [],
   "source": [
    "# Bleu Score\n",
    "bleu_score = corpus_bleu([[refer] for refer in reference_sentences], generated_sentences)\n",
    "print(\"Blue Score\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zc35rZjFyBHN"
   },
   "outputs": [],
   "source": [
    "#Rouge Score\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def calculate_ROUGE_score(ref_file, hyp_file):\n",
    "    # Read reference and hypothesis text from files\n",
    "    reference_text = read_text_file(ref_file)\n",
    "    hypothesis_text = read_text_file(hyp_file)\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "    # Calculate ROUGE scores\n",
    "    scores = scorer.score(hypothesis_text, reference_text)\n",
    "    return scores\n",
    "\n",
    "\n",
    "rouge_scores = calculate_ROUGE_score(reference_file,generated_file)\n",
    "print(\"Rouge Score\",rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiQVyh-dyJXu"
   },
   "source": [
    "# **Predict non-sarcastic data using non-sarcastic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-u6pIJgyMY-"
   },
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"output_mt5_gu_non_sar_m\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"output_mt5_gu_non_sar_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mqT_M9_yQD0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = \"gujarati_non_sarcastic_test_zeroM.csv\"  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract the \"title\" column\n",
    "titles = df[\"title\"]\n",
    "text = df[\"text\"]\n",
    "\n",
    "# Save the titles to a text file\n",
    "output_txt_file_path = \"ground_truth_gu_non_sar_title.txt\"  # Replace with the path to your output text file\n",
    "with open(output_txt_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for title in titles:\n",
    "        output_file.write(title + \"\\n\")\n",
    "\n",
    "output_txt_file_path = \"ground_truth_non_gu_sar_text.txt\"  # Replace with the path to your output text file\n",
    "with open(output_txt_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for title in titles:\n",
    "        output_file.write(title + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVNcZ2XryRNX"
   },
   "outputs": [],
   "source": [
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file.readlines()]\n",
    "    return sentences\n",
    "\n",
    "title_file = 'ground_truth_gu_non_sar_title.txt'\n",
    "text_file = 'ground_truth_non_gu_sar_text.txt'\n",
    "\n",
    "# Read sentences from files\n",
    "title_sentences = read_sentences_from_file(title_file)\n",
    "text_sentences = read_sentences_from_file(text_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIwcTCyjyVQv"
   },
   "outputs": [],
   "source": [
    "# Define input text as an array of sentences\n",
    "# input_texts = [\"કોરોના મહામારી સંકટ\", \"મહામારી\"]\n",
    "\n",
    "# List to store generated outputs\n",
    "output_texts = []\n",
    "\n",
    "# Process each sentence to generate output\n",
    "for input_text in text_sentences:\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # Decode output tokens\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Append output to the list\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "    # Print output text\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Output:\", output_text)\n",
    "    print()\n",
    "\n",
    "# Write output texts to a text file\n",
    "with open(\"generated_outputs_non.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for output_text in output_texts:\n",
    "        file.write(output_text + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldR8miBbyX5b"
   },
   "source": [
    "# **Calculate scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hvo8CsG3yaod"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = \"gujarati_non_sarcastic_test_zeroM.csv\"  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract the \"title\" column\n",
    "titles = df[\"title\"]\n",
    "\n",
    "# Save the titles to a text file\n",
    "output_txt_file_path = \"ground_truth_gu_non_sar.txt\"  # Replace with the path to your output text file\n",
    "with open(output_txt_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for title in titles:\n",
    "        output_file.write(title + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmSv6srBydSM"
   },
   "outputs": [],
   "source": [
    "# Open the text file for reading\n",
    "with open('generated_outputs_non.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read all lines from the file\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Modify each line to remove '<extra_id_0>'\n",
    "modified_lines = [line.replace('<extra_id_0> ', '') for line in lines]\n",
    "\n",
    "# Open the same text file for writing\n",
    "with open('generated_outputs_non.txt', 'w', encoding='utf-8') as file:\n",
    "    # Write the modified lines back to the file\n",
    "    file.writelines(modified_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HooRyRFmygLo"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file.readlines()]\n",
    "    return sentences\n",
    "\n",
    "generated_file = 'generated_outputs_non.txt'\n",
    "reference_file = 'ground_truth_gu_non_sar.txt'\n",
    "\n",
    "# Read sentences from files\n",
    "generated_sentences = read_sentences_from_file(generated_file)\n",
    "reference_sentences = read_sentences_from_file(reference_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPjzR1MByi7u"
   },
   "outputs": [],
   "source": [
    "# Bleu Score\n",
    "bleu_score = corpus_bleu([[refer] for refer in reference_sentences], generated_sentences)\n",
    "print(\"Bleu Score\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpQT4p-zykHY"
   },
   "outputs": [],
   "source": [
    "#Rouge Score\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def calculate_ROUGE_score(ref_file, hyp_file):\n",
    "    # Read reference and hypothesis text from files\n",
    "    reference_text = read_text_file(ref_file)\n",
    "    hypothesis_text = read_text_file(hyp_file)\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "    # Calculate ROUGE scores\n",
    "    scores = scorer.score(hypothesis_text, reference_text)\n",
    "    return scores\n",
    "\n",
    "\n",
    "rouge_scores = calculate_ROUGE_score(reference_file,generated_file)\n",
    "print(\"Rouge Score\",rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DFG_ZN3ysrZ"
   },
   "source": [
    "# **Predict sarcastic data using non-sarcastic Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EM60ZJnXyvOp"
   },
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"output_mt5_gu_non_sar_m\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"output_mt5_gu_non_sar_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxJutA1xyxp2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = \"gujarati_sarcastic_test_zeroM.csv\"  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract the \"title\" column\n",
    "titles = df[\"title\"]\n",
    "text = df[\"text\"]\n",
    "\n",
    "# Save the titles to a text file\n",
    "output_txt_file_path = \"ground_truth_gu_sar_title.txt\"  # Replace with the path to your output text file\n",
    "with open(output_txt_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for title in titles:\n",
    "        output_file.write(title + \"\\n\")\n",
    "\n",
    "output_txt_file_path = \"ground_truth_gu_sar_text.txt\"  # Replace with the path to your output text file\n",
    "with open(output_txt_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for title in titles:\n",
    "        output_file.write(title + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2IX8RVby0GE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file.readlines()]\n",
    "    return sentences\n",
    "\n",
    "title_file = 'ground_truth_gu_sar_title.txt'\n",
    "text_file = 'ground_truth_gu_sar_text.txt'\n",
    "\n",
    "# Read sentences from files\n",
    "title_sentences = read_sentences_from_file(title_file)\n",
    "text_sentences = read_sentences_from_file(text_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfLiZQNvy31G"
   },
   "outputs": [],
   "source": [
    "# Define input text as an array of sentences\n",
    "# input_texts = [\"કોરોના મહામારી સંકટ\", \"મહામારી\"]\n",
    "\n",
    "# List to store generated outputs\n",
    "output_texts = []\n",
    "\n",
    "# Process each sentence to generate output\n",
    "for input_text in text_sentences:\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    # Decode output tokens\n",
    "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Append output to the list\n",
    "    output_texts.append(output_text)\n",
    "\n",
    "    # Print output text\n",
    "    print(\"Input:\", input_text)\n",
    "    print(\"Output:\", output_text)\n",
    "    print()\n",
    "\n",
    "# Write output texts to a text file\n",
    "with open(\"generated_outputs_non_sar.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for output_text in output_texts:\n",
    "        file.write(output_text + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1MnSPD6y6P2"
   },
   "source": [
    "# **Calculate scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEk3WFppy4oH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = \"gujarati_sarcastic_test_zeroM.csv\"  # Replace with the path to your CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Extract the \"title\" column\n",
    "titles = df[\"title\"]\n",
    "\n",
    "# Save the titles to a text file\n",
    "output_txt_file_path = \"ground_truth_gu_sar.txt\"  # Replace with the path to your output text file\n",
    "with open(output_txt_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for title in titles:\n",
    "        output_file.write(title + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSQtf3dgy_Ms"
   },
   "outputs": [],
   "source": [
    "# Open the text file for reading\n",
    "with open('generated_outputs_non_sar.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read all lines from the file\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Modify each line to remove '<extra_id_0>'\n",
    "modified_lines = [line.replace('<extra_id_0> ', '') for line in lines]\n",
    "\n",
    "# Open the same text file for writing\n",
    "with open('generated_outputs_non_sar.txt', 'w', encoding='utf-8') as file:\n",
    "    # Write the modified lines back to the file\n",
    "    file.writelines(modified_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89kTFfhCzAHH"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_sentences_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentences = [line.strip() for line in file.readlines()]\n",
    "    return sentences\n",
    "\n",
    "generated_file = 'generated_outputs_non_sar.txt'\n",
    "reference_file = 'ground_truth_gu_sar.txt'\n",
    "\n",
    "# Read sentences from files\n",
    "generated_sentences = read_sentences_from_file(generated_file)\n",
    "reference_sentences = read_sentences_from_file(reference_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_-RdcPPzCLg"
   },
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nq3-iAknzGjo"
   },
   "outputs": [],
   "source": [
    "# Bleu Score\n",
    "bleu_score = corpus_bleu([[refer] for refer in reference_sentences], generated_sentences)\n",
    "print(\"Blue Score\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8B4aL8RYzIn6"
   },
   "outputs": [],
   "source": [
    "#Rouge Score\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "\n",
    "def calculate_ROUGE_score(ref_file, hyp_file):\n",
    "    # Read reference and hypothesis text from files\n",
    "    reference_text = read_text_file(ref_file)\n",
    "    hypothesis_text = read_text_file(hyp_file)\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeL'], use_stemmer=True)\n",
    "    # Calculate ROUGE scores\n",
    "    scores = scorer.score(hypothesis_text, reference_text)\n",
    "    return scores\n",
    "\n",
    "\n",
    "rouge_scores = calculate_ROUGE_score(reference_file,generated_file)\n",
    "print(\"Rouge Score\",rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-APhpKTHHvN"
   },
   "source": [
    "## **Telugu sarcastic model on sarcastic articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "do5C53A8HHvO",
    "outputId": "dcac0013-a876-4a76-85dd-d674efec8ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-23 14:34:51.277559: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-23 14:34:51.277604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-23 14:34:51.278949: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-23 14:34:51.286639: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 14:34:52.412055: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using custom data configuration default-32c7bdfe6279bb45\n",
      "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
      "Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-32c7bdfe6279bb45/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5)\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-32c7bdfe6279bb45/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 596 examples [00:00, 10954.50 examples/s]\n",
      "Generating test split\n",
      "Generating test split: 138 examples [00:00, 22495.68 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-32c7bdfe6279bb45/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5. Subsequent calls will reuse this data.\n",
      "config.json: 100% 553/553 [00:00<00:00, 3.43MB/s]\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 14:34:58,450 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 14:34:58,454 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100% 82.0/82.0 [00:00<00:00, 525kB/s]\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 14:34:58,694 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 14:34:58,696 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "spiece.model: 100% 4.31M/4.31M [00:00<00:00, 15.0MB/s]\n",
      "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 670kB/s]\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 14:34:59,658 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 14:34:59,658 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 14:34:59,658 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 14:34:59,658 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 14:34:59,658 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 14:34:59,658 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 14:34:59,659 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:329] 2024-04-23 14:35:00,158 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 14:35:01,078 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 14:35:01,079 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "pytorch_model.bin: 100% 1.20G/1.20G [00:06<00:00, 191MB/s]\n",
      "[INFO|modeling_utils.py:3431] 2024-04-23 14:35:09,240 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:936] 2024-04-23 14:35:09,293 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4172] 2024-04-23 14:35:12,772 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4180] 2024-04-23 14:35:12,773 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
      "generation_config.json: 100% 147/147 [00:00<00:00, 667kB/s]\n",
      "[INFO|configuration_utils.py:891] 2024-04-23 14:35:12,942 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/generation_config.json\n",
      "[INFO|configuration_utils.py:936] 2024-04-23 14:35:12,943 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Running tokenizer on train dataset:   0% 0/596 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-32c7bdfe6279bb45/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5/cache-6e004f036a75b016.arrow\n",
      "Running tokenizer on train dataset: 100% 596/596 [00:00<00:00, 1383.20 examples/s]\n",
      "Running tokenizer on prediction dataset:   0% 0/138 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-32c7bdfe6279bb45/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5/cache-0bf0c61a67dcc664.arrow\n",
      "Running tokenizer on prediction dataset: 100% 138/138 [00:00<00:00, 1597.07 examples/s]\n",
      "Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 23.1MB/s]\n",
      "[INFO|trainer.py:2048] 2024-04-23 14:35:15,213 >> ***** Running training *****\n",
      "[INFO|trainer.py:2049] 2024-04-23 14:35:15,213 >>   Num examples = 596\n",
      "[INFO|trainer.py:2050] 2024-04-23 14:35:15,213 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2051] 2024-04-23 14:35:15,213 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2054] 2024-04-23 14:35:15,213 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2055] 2024-04-23 14:35:15,213 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2056] 2024-04-23 14:35:15,213 >>   Total optimization steps = 447\n",
      "[INFO|trainer.py:2057] 2024-04-23 14:35:15,214 >>   Number of trainable parameters = 300,176,768\n",
      "100% 447/447 [01:58<00:00,  3.97it/s][INFO|trainer.py:2316] 2024-04-23 14:37:13,853 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100% 447/447 [01:58<00:00,  3.77it/s]\n",
      "[INFO|trainer.py:3305] 2024-04-23 14:37:13,855 >> Saving model checkpoint to output_mt5_tl/\n",
      "[INFO|configuration_utils.py:471] 2024-04-23 14:37:13,856 >> Configuration saved in output_mt5_tl/config.json\n",
      "[INFO|configuration_utils.py:705] 2024-04-23 14:37:13,857 >> Configuration saved in output_mt5_tl/generation_config.json\n",
      "[INFO|modeling_utils.py:2592] 2024-04-23 14:37:29,988 >> Model weights saved in output_mt5_tl/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2489] 2024-04-23 14:37:29,989 >> tokenizer config file saved in output_mt5_tl/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2498] 2024-04-23 14:37:29,989 >> Special tokens file saved in output_mt5_tl/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-04-23 14:37:30,023 >> Copy vocab file to output_mt5_tl/spiece.model\n",
      "[INFO|trainer.py:3614] 2024-04-23 14:37:33,931 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3616] 2024-04-23 14:37:33,931 >>   Num examples = 138\n",
      "[INFO|trainer.py:3619] 2024-04-23 14:37:33,931 >>   Batch size = 8\n",
      "100% 18/18 [00:36<00:00,  2.01s/it]\n",
      "[INFO|modelcard.py:450] 2024-04-23 14:38:13,780 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path google/mt5-small \\\n",
    "    --do_train True \\\n",
    "    --do_eval False \\\n",
    "    --do_predict True \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --train_file telugu_sarcastic_train.csv \\\n",
    "    --test_file tl_sarcastic_articles_test.csv \\\n",
    "    --text_column \"text\" \\\n",
    "    --summary_column \"title\" \\\n",
    "    --max_target_length 298 \\\n",
    "    --output_dir output_mt5_tl/ \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --logging_strategy \"epoch\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --overwrite_output_dir True \\\n",
    "    --predict_with_generate $@ 2>&1>./hg_mt5_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh5yIUzhHHvO",
    "outputId": "48bafe33-1934-46bf-caab-7854d959f5ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "whCOQMYhHHvO"
   },
   "outputs": [],
   "source": [
    "!cp -r output_mt5_tl /content/drive/My\\ Drive/saved_trainer/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtzSIuUFHHvO",
    "outputId": "1f8cd417-7aeb-4beb-c50f-38909c83a60c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1156: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <extra_id_0>. 'నీకు కావాలంటే' అని మీ అమ్మ రాసింది.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"output_mt5_tl\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"output_mt5_tl\")\n",
    "\n",
    "# Define input text\n",
    "input_text = \"పోర్ట్‌ల్యాండ్, మైనే - ఈ ఉదయమే దీన్ని తపాలా కార్యాలయంలో వేశానని, నువ్వు అందుకోవాలని మీ అమ్మ శనివారం తెలియజేసింది, నివేదికలు ధృవీకరించాయి. 'నీకు ఏదో పంపుతున్నాను' అని మీ అమ్మ నుండి ఒక టెక్స్ట్ సందేశం వచ్చింది. 'ఏదో కొన్ని రోజుల్లో అందుకుంటావులే' అని తెలిపింది. 'నీకిది వచ్చాక నాకు చెప్పాలి' అని మీ అమ్మ రాసింది. ప్రెస్ టైమ్ లో, మీ అమ్మ 'నీకు కావాలంటే' అని ప్యాకేజ్ ట్రాకింగ్ నంబర్‌ను మీకు ఈమెయిల్ చేసింది.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode output tokens\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print output text\n",
    "print(\"Output:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "fbMy2rcUcM5V"
   },
   "outputs": [],
   "source": [
    "with open('tl_sarcastic_vs_sarcastic_mt5.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = [line.replace('<extra_id_0>', '') for line in lines]\n",
    "\n",
    "with open('tl_sarcastic_vs_sarcastic_mt5_trained_cleaned.txt', 'w', encoding='utf-8') as file:\n",
    "    file.writelines(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "_ilPLVSZbbrp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('tl_sarcastic_articles_test.csv')\n",
    "\n",
    "# Extract the 'title' column\n",
    "titles = df['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m3_75AuScWtW",
    "outputId": "3ab340e9-e075-4d02-fe7f-ef94992223e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.0068536145655216886\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Example ground truth and model-generated outputs as lists of strings\n",
    "ground_truth = cleaned_lines\n",
    "model_output = titles\n",
    "\n",
    "# Convert strings to lists of tokens\n",
    "ground_truth_tokenized = [reference.split() for reference in ground_truth]\n",
    "model_output_tokenized = [hypothesis.split() for hypothesis in model_output]\n",
    "\n",
    "# Calculate the BLEU score\n",
    "bleu_score = corpus_bleu([[reference] for reference in ground_truth_tokenized], model_output_tokenized)\n",
    "\n",
    "print(f\"BLEU score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "xAVYxb7cehRr"
   },
   "outputs": [],
   "source": [
    "model_output=list(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nA1oxnxquIf-",
    "outputId": "974d1607-1190-4653-a76d-7ec7da264b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.09686167929875023, 'p': 0.07364541658019916, 'f': 0.07851776803725381}, 'rouge-2': {'r': 0.01920002300437083, 'p': 0.01739138478268913, 'f': 0.017370426123417017}, 'rouge-l': {'r': 0.09533188863852479, 'p': 0.07180506691376254, 'f': 0.07684919442932092}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Example ground truth and model-generated outputs as lists of strings\n",
    "ground_truth = cleaned_lines\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "rouge = Rouge()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = rouge.get_scores(model_output, ground_truth, avg=True)\n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5aqUuTGe2f2"
   },
   "source": [
    "## **Telugu Non sarcastic model on sarcastic articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bif5VFE2e2gB",
    "outputId": "75279efc-3324-4e2c-ad5e-f7a00ae71cb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-23 15:04:40.100029: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-23 15:04:40.100082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-23 15:04:40.101575: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-23 15:04:40.109731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 15:04:41.292541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using custom data configuration default-b45e9853f2638364\n",
      "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
      "Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-b45e9853f2638364/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5)\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-b45e9853f2638364/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 606 examples [00:00, 13737.40 examples/s]\n",
      "Generating test split\n",
      "Generating test split: 138 examples [00:00, 21917.30 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-b45e9853f2638364/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 15:04:45,175 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 15:04:45,178 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 15:04:45,268 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 15:04:45,269 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:04:45,269 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:04:45,269 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:04:45,270 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:04:45,270 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:04:45,270 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 15:04:45,270 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 15:04:45,271 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:329] 2024-04-23 15:04:45,768 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 15:04:46,799 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 15:04:46,800 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:3431] 2024-04-23 15:04:48,150 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:936] 2024-04-23 15:04:48,201 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4172] 2024-04-23 15:04:56,373 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4180] 2024-04-23 15:04:56,373 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:891] 2024-04-23 15:04:56,466 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/generation_config.json\n",
      "[INFO|configuration_utils.py:936] 2024-04-23 15:04:56,466 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Running tokenizer on train dataset:   0% 0/606 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b45e9853f2638364/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5/cache-c3db83d510951cc1.arrow\n",
      "Running tokenizer on train dataset: 100% 606/606 [00:00<00:00, 1353.68 examples/s]\n",
      "Running tokenizer on prediction dataset:   0% 0/138 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-b45e9853f2638364/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5/cache-7d699f68272fdea7.arrow\n",
      "Running tokenizer on prediction dataset: 100% 138/138 [00:00<00:00, 1708.21 examples/s]\n",
      "[INFO|trainer.py:2048] 2024-04-23 15:04:58,643 >> ***** Running training *****\n",
      "[INFO|trainer.py:2049] 2024-04-23 15:04:58,643 >>   Num examples = 606\n",
      "[INFO|trainer.py:2050] 2024-04-23 15:04:58,643 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2051] 2024-04-23 15:04:58,643 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2054] 2024-04-23 15:04:58,643 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2055] 2024-04-23 15:04:58,643 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2056] 2024-04-23 15:04:58,643 >>   Total optimization steps = 456\n",
      "[INFO|trainer.py:2057] 2024-04-23 15:04:58,644 >>   Number of trainable parameters = 300,176,768\n",
      "100% 456/456 [02:02<00:00,  4.24it/s][INFO|trainer.py:2316] 2024-04-23 15:07:01,108 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100% 456/456 [02:02<00:00,  3.72it/s]\n",
      "[INFO|trainer.py:3305] 2024-04-23 15:07:01,111 >> Saving model checkpoint to output_mt5_tl_nonsarcastic/\n",
      "[INFO|configuration_utils.py:471] 2024-04-23 15:07:01,113 >> Configuration saved in output_mt5_tl_nonsarcastic/config.json\n",
      "[INFO|configuration_utils.py:705] 2024-04-23 15:07:01,114 >> Configuration saved in output_mt5_tl_nonsarcastic/generation_config.json\n",
      "[INFO|modeling_utils.py:2592] 2024-04-23 15:07:12,410 >> Model weights saved in output_mt5_tl_nonsarcastic/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2489] 2024-04-23 15:07:12,411 >> tokenizer config file saved in output_mt5_tl_nonsarcastic/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2498] 2024-04-23 15:07:12,412 >> Special tokens file saved in output_mt5_tl_nonsarcastic/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-04-23 15:07:12,435 >> Copy vocab file to output_mt5_tl_nonsarcastic/spiece.model\n",
      "[INFO|trainer.py:3614] 2024-04-23 15:07:12,503 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3616] 2024-04-23 15:07:12,503 >>   Num examples = 138\n",
      "[INFO|trainer.py:3619] 2024-04-23 15:07:12,503 >>   Batch size = 8\n",
      "100% 18/18 [00:24<00:00,  1.37s/it]\n",
      "[INFO|modelcard.py:450] 2024-04-23 15:07:38,662 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path google/mt5-small \\\n",
    "    --do_train True \\\n",
    "    --do_eval False \\\n",
    "    --do_predict True \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --train_file telugu_non_sarcastic_train.csv \\\n",
    "    --test_file tl_sarcastic_articles_test.csv \\\n",
    "    --text_column \"text\" \\\n",
    "    --summary_column \"title\" \\\n",
    "    --max_target_length 298 \\\n",
    "    --output_dir output_mt5_tl_nonsarcastic/ \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --logging_strategy \"epoch\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --overwrite_output_dir True \\\n",
    "    --predict_with_generate $@ 2>&1>./hg_mt5_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vc5QWucRe2gB",
    "outputId": "19b80e7e-43fa-4bee-bb8e-8e7292f8b562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AzcbpW-Ne2gB"
   },
   "outputs": [],
   "source": [
    "!cp -r output_mt5_tl_nonsarcastic /content/drive/My\\ Drive/saved_non_sarcastictrainer/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16VZEM1ze2gB",
    "outputId": "823296c3-f185-4334-b7e6-dac2ffa13d9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1156: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <extra_id_0>. 'నీకు కావాలంటే' అని మీ అమ్మ రాసింది.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"output_mt5_tl\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"output_mt5_tl\")\n",
    "\n",
    "# Define input text\n",
    "input_text = \"పోర్ట్‌ల్యాండ్, మైనే - ఈ ఉదయమే దీన్ని తపాలా కార్యాలయంలో వేశానని, నువ్వు అందుకోవాలని మీ అమ్మ శనివారం తెలియజేసింది, నివేదికలు ధృవీకరించాయి. 'నీకు ఏదో పంపుతున్నాను' అని మీ అమ్మ నుండి ఒక టెక్స్ట్ సందేశం వచ్చింది. 'ఏదో కొన్ని రోజుల్లో అందుకుంటావులే' అని తెలిపింది. 'నీకిది వచ్చాక నాకు చెప్పాలి' అని మీ అమ్మ రాసింది. ప్రెస్ టైమ్ లో, మీ అమ్మ 'నీకు కావాలంటే' అని ప్యాకేజ్ ట్రాకింగ్ నంబర్‌ను మీకు ఈమెయిల్ చేసింది.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode output tokens\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print output text\n",
    "print(\"Output:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "RzoaBQhye2gC"
   },
   "outputs": [],
   "source": [
    "with open('tl_sarcastic_vs_non_sarcastic_mt5.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = [line.replace('<extra_id_0>', '') for line in lines]\n",
    "\n",
    "with open('tl_sarcastic_vs_non_sarcastic_mt5_trained_cleaned.txt', 'w', encoding='utf-8') as file:\n",
    "    file.writelines(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jt2zuRXXe2gD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('tl_sarcastic_articles_test.csv')\n",
    "\n",
    "# Extract the 'title' column\n",
    "titles = df['title']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cAUJ3lhe2gD",
    "outputId": "9be6a731-1bc8-4671-bb35-3d2dabfa082a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.013472341009816211\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Example ground truth and model-generated outputs as lists of strings\n",
    "ground_truth = cleaned_lines\n",
    "model_output = titles\n",
    "\n",
    "# Convert strings to lists of tokens\n",
    "ground_truth_tokenized = [reference.split() for reference in ground_truth]\n",
    "model_output_tokenized = [hypothesis.split() for hypothesis in model_output]\n",
    "\n",
    "# Calculate the BLEU score\n",
    "bleu_score = corpus_bleu([[reference] for reference in ground_truth_tokenized], model_output_tokenized)\n",
    "\n",
    "print(f\"BLEU score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "W7Ix5xMEta1C"
   },
   "outputs": [],
   "source": [
    "model_output=list(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MvqQowStFt5",
    "outputId": "4c0a6046-a4ca-4165-daf2-0a7f10cb8870"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.875, 'p': 0.8, 'f': 0.8333333283641975}, 'rouge-2': {'r': 0.6666666666666666, 'p': 0.625, 'f': 0.6428571379081633}, 'rouge-l': {'r': 0.875, 'p': 0.8, 'f': 0.8333333283641975}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Example ground truth and model-generated outputs as lists of strings\n",
    "ground_truth = ['this is a test', 'this is another test']\n",
    "model_output = ['this is a test', 'this is a different test']\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "rouge = Rouge()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = rouge.get_scores(model_output, ground_truth, avg=True)\n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqjdbOb-miR0"
   },
   "source": [
    "## **Telugu Non sarcastic model on non sarcastic articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z6l5mO4hmiSA",
    "outputId": "adba4283-4e1e-4013-d58d-ba8b1bdaeaef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-23 15:42:10.532684: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-23 15:42:10.532738: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-23 15:42:10.534725: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-23 15:42:10.551569: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 15:42:12.638481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using custom data configuration default-e96fb76b8fc0e9ff\n",
      "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/csv\n",
      "Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-e96fb76b8fc0e9ff/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5)\n",
      "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-e96fb76b8fc0e9ff/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5...\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "Generating train split: 606 examples [00:00, 6492.80 examples/s]\n",
      "Generating test split\n",
      "Generating test split: 140 examples [00:00, 7260.35 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-e96fb76b8fc0e9ff/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5. Subsequent calls will reuse this data.\n",
      "config.json: 100% 553/553 [00:00<00:00, 2.48MB/s]\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 15:42:22,403 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 15:42:22,413 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "tokenizer_config.json: 100% 82.0/82.0 [00:00<00:00, 384kB/s]\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 15:42:22,627 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 15:42:22,642 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "spiece.model: 100% 4.31M/4.31M [00:00<00:00, 11.5MB/s]\n",
      "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 428kB/s]\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:42:23,627 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:42:23,628 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:42:23,628 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:42:23,628 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2088] 2024-04-23 15:42:23,628 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 15:42:23,628 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 15:42:23,630 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "[WARNING|logging.py:329] 2024-04-23 15:42:24,850 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "[INFO|configuration_utils.py:726] 2024-04-23 15:42:26,899 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/config.json\n",
      "[INFO|configuration_utils.py:789] 2024-04-23 15:42:26,900 >> Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-small\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.41.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "pytorch_model.bin: 100% 1.20G/1.20G [00:09<00:00, 127MB/s] \n",
      "[INFO|modeling_utils.py:3431] 2024-04-23 15:42:38,568 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/pytorch_model.bin\n",
      "[INFO|configuration_utils.py:936] 2024-04-23 15:42:38,624 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4172] 2024-04-23 15:42:42,420 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4180] 2024-04-23 15:42:42,420 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
      "generation_config.json: 100% 147/147 [00:00<00:00, 694kB/s]\n",
      "[INFO|configuration_utils.py:891] 2024-04-23 15:42:42,600 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-small/snapshots/73fb5dbe4756edadc8fbe8c769b0a109493acf7a/generation_config.json\n",
      "[INFO|configuration_utils.py:936] 2024-04-23 15:42:42,601 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "Running tokenizer on train dataset:   0% 0/606 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e96fb76b8fc0e9ff/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5/cache-aa870518d64b885e.arrow\n",
      "Running tokenizer on train dataset: 100% 606/606 [00:00<00:00, 1330.32 examples/s]\n",
      "Running tokenizer on prediction dataset:   0% 0/140 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-e96fb76b8fc0e9ff/0.0.0/8d73bd761341cee405ddc715f0eebe400df876d7da154d3a2263a460648d6ba5/cache-b4531558d62622c4.arrow\n",
      "Running tokenizer on prediction dataset: 100% 140/140 [00:00<00:00, 1221.57 examples/s]\n",
      "Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 21.4MB/s]\n",
      "[INFO|trainer.py:2048] 2024-04-23 15:42:45,861 >> ***** Running training *****\n",
      "[INFO|trainer.py:2049] 2024-04-23 15:42:45,861 >>   Num examples = 606\n",
      "[INFO|trainer.py:2050] 2024-04-23 15:42:45,861 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2051] 2024-04-23 15:42:45,861 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2054] 2024-04-23 15:42:45,861 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2055] 2024-04-23 15:42:45,861 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2056] 2024-04-23 15:42:45,861 >>   Total optimization steps = 456\n",
      "[INFO|trainer.py:2057] 2024-04-23 15:42:45,862 >>   Number of trainable parameters = 300,176,768\n",
      "100% 456/456 [02:07<00:00,  4.43it/s][INFO|trainer.py:2316] 2024-04-23 15:44:53,699 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100% 456/456 [02:07<00:00,  3.57it/s]\n",
      "[INFO|trainer.py:3305] 2024-04-23 15:44:53,701 >> Saving model checkpoint to output_mt5_tl_nonsarcastic_and_ns/\n",
      "[INFO|configuration_utils.py:471] 2024-04-23 15:44:53,703 >> Configuration saved in output_mt5_tl_nonsarcastic_and_ns/config.json\n",
      "[INFO|configuration_utils.py:705] 2024-04-23 15:44:53,703 >> Configuration saved in output_mt5_tl_nonsarcastic_and_ns/generation_config.json\n",
      "[INFO|modeling_utils.py:2592] 2024-04-23 15:45:14,166 >> Model weights saved in output_mt5_tl_nonsarcastic_and_ns/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2489] 2024-04-23 15:45:14,167 >> tokenizer config file saved in output_mt5_tl_nonsarcastic_and_ns/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2498] 2024-04-23 15:45:14,167 >> Special tokens file saved in output_mt5_tl_nonsarcastic_and_ns/special_tokens_map.json\n",
      "[INFO|tokenization_t5_fast.py:176] 2024-04-23 15:45:14,171 >> Copy vocab file to output_mt5_tl_nonsarcastic_and_ns/spiece.model\n",
      "[INFO|trainer.py:3614] 2024-04-23 15:45:14,218 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:3616] 2024-04-23 15:45:14,218 >>   Num examples = 140\n",
      "[INFO|trainer.py:3619] 2024-04-23 15:45:14,218 >>   Batch size = 8\n",
      "100% 18/18 [00:34<00:00,  1.90s/it]\n",
      "[INFO|modelcard.py:450] 2024-04-23 15:45:50,229 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"
     ]
    }
   ],
   "source": [
    "!python transformers/examples/pytorch/summarization/run_summarization.py \\\n",
    "    --model_name_or_path google/mt5-small \\\n",
    "    --do_train True \\\n",
    "    --do_eval False \\\n",
    "    --do_predict True \\\n",
    "    --source_prefix \"summarize: \" \\\n",
    "    --train_file telugu_non_sarcastic_train.csv \\\n",
    "    --test_file tl_non_sarcastic_articles_test.csv \\\n",
    "    --text_column \"text\" \\\n",
    "    --summary_column \"title\" \\\n",
    "    --max_target_length 298 \\\n",
    "    --output_dir output_mt5_tl_nonsarcastic_and_ns/ \\\n",
    "    --per_device_train_batch_size=4 \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --logging_strategy \"epoch\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --overwrite_output_dir True \\\n",
    "    --predict_with_generate $@ 2>&1>./hg_mt5_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2suI9l7miSB",
    "outputId": "ec772317-9c86-4deb-b344-6b8085821bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Uxs4b7TVmiSB"
   },
   "outputs": [],
   "source": [
    "!cp -r output_mt5_tl_nonsarcastic_and_ns /content/drive/My\\ Drive/saved_non_sarcastictrainer_and_ns/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KObabDezmiSB",
    "outputId": "895f5b6f-b698-4aba-d601-208d22cbcda4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1156: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <extra_id_0> కోసం పంపుతున్నాను.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"output_mt5_tl_nonsarcastic_and_ns\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"output_mt5_tl_nonsarcastic_and_ns\")\n",
    "\n",
    "# Define input text\n",
    "input_text = \"పోర్ట్‌ల్యాండ్, మైనే - ఈ ఉదయమే దీన్ని తపాలా కార్యాలయంలో వేశానని, నువ్వు అందుకోవాలని మీ అమ్మ శనివారం తెలియజేసింది, నివేదికలు ధృవీకరించాయి. 'నీకు ఏదో పంపుతున్నాను' అని మీ అమ్మ నుండి ఒక టెక్స్ట్ సందేశం వచ్చింది. 'ఏదో కొన్ని రోజుల్లో అందుకుంటావులే' అని తెలిపింది. 'నీకిది వచ్చాక నాకు చెప్పాలి' అని మీ అమ్మ రాసింది. ప్రెస్ టైమ్ లో, మీ అమ్మ 'నీకు కావాలంటే' అని ప్యాకేజ్ ట్రాకింగ్ నంబర్‌ను మీకు ఈమెయిల్ చేసింది.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode output tokens\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print output text\n",
    "print(\"Output:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-SY6vyKDmiSC"
   },
   "outputs": [],
   "source": [
    "with open('tl_non_sarcastic_vs_non_sarcastic_mt5.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "cleaned_lines = [line.replace('<extra_id_0>', '') for line in lines]\n",
    "\n",
    "with open('tl_non_sarcastic_vs_non_sarcastic_mt5_trained_cleaned.txt', 'w', encoding='utf-8') as file:\n",
    "    file.writelines(cleaned_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "JsIBI7uamiSC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('tl_non_sarcastic_articles_test.csv')\n",
    "\n",
    "# Extract the 'title' column\n",
    "titles = list(df['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LGOR5wlymiSC",
    "outputId": "fb690d7f-2101-42dd-ac83-65de09de6b0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.007293432929988352\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Example ground truth and model-generated outputs as lists of strings\n",
    "ground_truth = cleaned_lines\n",
    "model_output = titles\n",
    "\n",
    "# Convert strings to lists of tokens\n",
    "ground_truth_tokenized = [reference.split() for reference in ground_truth]\n",
    "model_output_tokenized = [hypothesis.split() for hypothesis in model_output]\n",
    "\n",
    "# Calculate the BLEU score\n",
    "bleu_score = corpus_bleu([[reference] for reference in ground_truth_tokenized], model_output_tokenized)\n",
    "\n",
    "print(f\"BLEU score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5hfxXcUqp49",
    "outputId": "480ce954-c1dc-4f99-9119-17ebb087e3f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge-1': {'r': 0.0947597442240299, 'p': 0.07618976261833404, 'f': 0.08030237757727998}, 'rouge-2': {'r': 0.019254792826221393, 'p': 0.014123277516134654, 'f': 0.015853166066138503}, 'rouge-l': {'r': 0.09404545850974419, 'p': 0.07539611182468325, 'f': 0.07955049787803187}}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "# Example ground truth and model-generated outputs as lists of strings\n",
    "ground_truth = cleaned_lines\n",
    "model_output = titles\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "rouge = Rouge()\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = rouge.get_scores(model_output, ground_truth, avg=True)\n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYhjtM-trTqE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3MDwSsuxMi3"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
